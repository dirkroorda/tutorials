{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dans](images/dans.png)\n",
    "![tf](images/tf-small.png)\n",
    "\n",
    "# Turn your corpus into a Text-Fabric dataset\n",
    "\n",
    "## Corpus\n",
    "\n",
    "We start with a baby corpus:\n",
    "\n",
    "1 book, with 2 chapters, each having one or two sentences.\n",
    "\n",
    "Here is the complete corpus source material:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '''\n",
    "# Consider Phlebas\n",
    "$ author=Iain M. Banks\n",
    "\n",
    "## 1\n",
    "Everything about us,\n",
    "everything around us,\n",
    "everything we know [and can know of] is composed ultimately of patterns of nothing;\n",
    "that’s the bottom line, the final truth.\n",
    "\n",
    "So where we find we have any control over those patterns,\n",
    "why not make the most elegant ones, the most enjoyable and good ones,\n",
    "in our own terms?\n",
    "\n",
    "## 2\n",
    "Besides,\n",
    "it left the humans in the Culture free to take care of the things that really mattered in life,\n",
    "such as [sports, games, romance,] studying dead languages,\n",
    "barbarian societies and impossible problems,\n",
    "and climbing high mountains without the aid of a safety harness.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a few details:\n",
    "\n",
    "* `#` marks the title of a book, section level 1.\n",
    "* `##` marks the number of a chapter, section level 2.\n",
    "* `$` starts a line with key=value metadata: the author.\n",
    "* *blank lines* split sentences.\n",
    "  There are 2 sentences in the first chapter and 1 in the second one.\n",
    "* We will give each sentence a number within its section.\n",
    "* The sentences are divided into lines.\n",
    "* We will give each line a number within its sentence.\n",
    "* Words within [ ] will not be part of the line, the line has a gap.\n",
    "* The gapped words will have a feature `gap=1`.\n",
    "* Lines will be split into words, the slot nodes.\n",
    "* We separate the word from its punctuation, which gets added in a `punc` feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up\n",
    "\n",
    "Now we start the engines: Text-Fabric, and the *walker* conversion module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from tf.fabric import Fabric\n",
    "from tf.convert.walker import CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call up TF and let it look into the directory where the output has to land,\n",
    "in this case your download directory.\n",
    "It will not like what it sees, but that is not our concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.3\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "10 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF_DIR = os.path.expanduser('~/Downloads/banks/tf')\n",
    "\n",
    "TF = Fabric(locations=TF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the conversion machinery: we obtain an object with methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CV(TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF configuration\n",
    "\n",
    "A Text-Fabric dataset is a bunch of individual `.tf` files that start with a little bit of metadata and then contain \n",
    "a stream of data, typically the values of a single feature for each node or edge in the graph.\n",
    "\n",
    "We specify the metadata bit by bit.\n",
    "\n",
    "### slot type\n",
    "\n",
    "A crucial design aspect of each TF dataset is its granularity. What are the slots?\n",
    "\n",
    "Words, morphemes, characters?\n",
    "\n",
    "You decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slotType = 'word'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provenance\n",
    "\n",
    "Users that encounter your tf data in the wild, will be thankful to you if you took the\n",
    "trouble to say in a few key-value pairs what this is about.\n",
    "\n",
    "The metadata you specify here will end up in all generated tf features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic = {\n",
    "    'name': 'Culture quotes from Iain Banks',\n",
    "    'compiler': 'Dirk Roorda',\n",
    "    'source': 'Good Reads',\n",
    "    'url': 'https://www.goodreads.com/work/quotes/14366-consider-phlebas',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text matters\n",
    "\n",
    "A few things concerning the presentation of your text can be specified in the `otext` feature.\n",
    "This is a tf feature without data, it has only metadata.\n",
    "\n",
    "It contains the specs for the section structure of your corpus and the text formats.\n",
    "\n",
    "#### Section structure\n",
    "\n",
    "Currently, TF assumes that there are two or three section levels.\n",
    "For each level you have to specify the corresponding node type and the feature that contains the section name or number.\n",
    "\n",
    "#### Text formats\n",
    "\n",
    "When you ask TF to render slot nodes (the nodes with text), TF needs to know\n",
    "which features to render. \n",
    "\n",
    "A text format is a template with placeholders for the features you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "otext = {\n",
    "    'fmt:text-orig-full': '{letters}{punc} ',\n",
    "    'sectionTypes': 'book,chapter',\n",
    "    'sectionFeatures': 'title,number',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typing\n",
    "\n",
    "The values of features are usually strings.\n",
    "But if you know that they are always integers (or None), you can declare a feature as an integer valued feature.\n",
    "\n",
    "The only thing you have to do is to declare them in the following set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intFeatures = {\n",
    "  'number',\n",
    "  'gap'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions\n",
    "\n",
    "You can say per feature what it does.\n",
    "Use as many key-values as you like.\n",
    "\n",
    "A good *description* is particularly helpful.\n",
    "\n",
    "It is surprising how often you want to consult those descriptions yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureMeta = {\n",
    "    'number': {\n",
    "        'description': 'number of chapter, or sentence in chapter, or line in sentence',\n",
    "    },\n",
    "    'gap': {\n",
    "        'description': '1 for words that occur between [ ]',\n",
    "    },\n",
    "    'title': {\n",
    "        'description': 'the title of a book',\n",
    "    },\n",
    "    'author': {\n",
    "        'description': 'the author of a book',\n",
    "    },\n",
    "    'terminator': {\n",
    "        'description': 'the last character of a line',\n",
    "    },\n",
    "    'letters': {\n",
    "        'description': 'the letters of a word',\n",
    "    },\n",
    "    'punc': {\n",
    "        'description': 'the punctuation after a word',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Director\n",
    "\n",
    "This is the heart of the matter.\n",
    "\n",
    "You program the director so that is unwraps your source material.\n",
    "\n",
    "Every time the director encounters a new textual object in the source,\n",
    "it issues an action requesting a new node.\n",
    "The director gets a receipt, by which it can issue subsequent\n",
    "actions for that node, like adding feature values to it.\n",
    "\n",
    "And when the object is done, the director issues a `terminate` action.\n",
    "\n",
    "During all this, the `cv` machine is busy to translate these actions into\n",
    "the construction of a proper TF graph representing all the\n",
    "source material that you have exposed to it.\n",
    "\n",
    "A few things to note\n",
    "\n",
    "* If you want to terminate a node, you do not have to worry whether the node exists or has already\n",
    "  been terminated: just do it\n",
    "* If you have terminated a node, you can still resume it\n",
    "* When you add nodes, slot and non-slots, there is magic behind the scenes:\n",
    "  * when a slot node is added, it will be linked to all active non-slot nodes,\n",
    "    i.e. the ones that have not been terminated or have been resumed;\n",
    "  * when a non slot node is added, is becomes automatically active,\n",
    "    in the sense that it will be linked to subsequent slot nodes, before it is terminated,\n",
    "    or after it has been resumed.\n",
    "* If a fatal error is encountered, the director can simply say `cv.stop(message)`.\n",
    "* After the director is done, TF will perform several checks on the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def director(cv):\n",
    "  counter = dict(\n",
    "    sentence=0,\n",
    "    line=0,\n",
    "  )\n",
    "  cur = dict(\n",
    "    book=None,\n",
    "    chapter=None,\n",
    "    sentence=None,\n",
    "  )\n",
    "\n",
    "  wordRe = re.compile(r'^(.*?)([^A-Za-z0-9]*)$')\n",
    "  metaRe = re.compile(r'^\\$\\s*([^= ]+)\\s*=\\s*(.*)')\n",
    "\n",
    "  for line in source.strip().split('\\n'):\n",
    "    line = line.rstrip()\n",
    "    if not line:\n",
    "      cv.terminate(cur['sentence'])              # action\n",
    "      for ntp in counter:\n",
    "        counter[ntp] += 1\n",
    "      cur['sentence'] = cv.node('sentence')      # action\n",
    "      cv.feature(\n",
    "        cur['sentence'],\n",
    "        number=counter['sentence'],\n",
    "      )                                          # action\n",
    "      continue\n",
    "      \n",
    "    if line.startswith('# '):\n",
    "      for ntp in ('sentence', 'chapter', 'book'):\n",
    "        cv.terminate(cur[ntp])                   # action\n",
    "        cur[ntp] = None         \n",
    "      title = line[2:].strip()\n",
    "      cur['book'] = cv.node('book')              # action\n",
    "      for ntp in counter:\n",
    "        counter[ntp] = 0\n",
    "      cv.feature(\n",
    "        cur['book'],\n",
    "        title=title,\n",
    "      )                                          # action\n",
    "      continue\n",
    "\n",
    "    if line.startswith('## '):\n",
    "      for ntp in ('sentence', 'chapter'):\n",
    "        cv.terminate(cur[ntp])                   # action\n",
    "        cur[ntp] = None         \n",
    "      number = line[2:].strip()\n",
    "      cur['chapter'] = cv.node('chapter')        # action\n",
    "      for ntp in counter:\n",
    "        counter[ntp] = 0\n",
    "      cv.feature(\n",
    "        cur['chapter'],\n",
    "        number=number,\n",
    "      )                                          # action\n",
    "      continue\n",
    "\n",
    "    if line.startswith('$'):\n",
    "      match = metaRe.match(line)\n",
    "      if not match:\n",
    "        cv.stop(f'Malformed metadata line: \"{line}\"') # action\n",
    "        return\n",
    "      name = match.group(1)\n",
    "      value = match.group(2)\n",
    "      cv.feature(\n",
    "        cur['book'],\n",
    "        **{name: value},\n",
    "      )                                           # action\n",
    "      continue\n",
    "        \n",
    "    if not cur['sentence']:\n",
    "      cur['sentence'] = cv.node('sentence')       # action\n",
    "      counter['sentence'] += 1\n",
    "      cv.feature(\n",
    "        cur['sentence'],\n",
    "        number=counter['sentence'],\n",
    "      )                                           # action\n",
    "      \n",
    "    cur['line'] = cv.node('line')                 # action\n",
    "    counter['line'] += 1\n",
    "    cv.feature(\n",
    "      cur['line'],\n",
    "      terminator=line[-1],\n",
    "      number=counter['line'],\n",
    "    )                                              # action\n",
    "    \n",
    "    gap = False\n",
    "    for word in line.split():\n",
    "      if word.startswith('['):\n",
    "        gap = True\n",
    "        cv.terminate(cur['line'])   # action\n",
    "        w = cv.slot()               # action\n",
    "        cv.feature(w, gap=1)        # action\n",
    "        word = word[1:]\n",
    "      elif word.endswith(']'):\n",
    "        w = cv.slot()               # action\n",
    "        cv.resume(cur['line'])      # action\n",
    "        cv.feature(w, gap=1)        # action\n",
    "        gap = False\n",
    "        word = word[0:-1]\n",
    "      else:\n",
    "        w = cv.slot()\n",
    "        if gap:\n",
    "          cv.feature(w, gap=1)      # action\n",
    "\n",
    "      (letters, punc) = wordRe.findall(word)[0]\n",
    "      cv.feature(w, letters=letters)            # action\n",
    "      if punc:\n",
    "        cv.feature(w, punc=punc)                # action\n",
    "    cv.terminate(cur['line'])                   # action\n",
    "    curLine = None\n",
    "    \n",
    "  for ntp in ('sentence', 'chapter', 'book'):\n",
    "    cv.terminate(cur[ntp])                      # action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "We are going to run the conversion and check whether all is well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |   SECTION TYPES:    book, chapter\n",
      "   |   SECTION FEATURES: title, number\n",
      "   |   TEXT    FEATURES:\n",
      "   |      |   text-orig-full       letters, punc\n",
      "   |     0.00s OK\n",
      "   |     0.00s Following director... \n",
      "   |     0.00s \"edge\" actions: 0\n",
      "   |     0.00s \"feature\" actions: 144\n",
      "   |     0.00s \"node\" actions: 20\n",
      "   |     0.00s \"resume\" actions: 2\n",
      "   |     0.00s \"slot\" actions: 99\n",
      "   |     0.00s \"terminate\" actions: 27\n",
      "   |          1 x \"book\" node \n",
      "   |          2 x \"chapter\" node \n",
      "   |         12 x \"line\" node \n",
      "   |          5 x \"sentence\" node \n",
      "   |         99 x \"word\" node  = slot type\n",
      "   |        119 nodes of all types\n",
      "   |     0.01s OK\n",
      "   |     0.00s Removing unlinked nodes ... \n",
      "   |      |    -0.00s      2 unlinked \"sentence\" nodes: [1, 4]\n",
      "   |      |     0.00s      2 unlinked nodes\n",
      "   |      |     0.00s Leaving    117 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.00s Sorting 1 nodes of type \"book\"\n",
      "   |     0.00s Sorting 2 nodes of type \"chapter\"\n",
      "   |     0.00s Sorting 12 nodes of type \"line\"\n",
      "   |     0.00s Sorting 3 nodes of type \"sentence\"\n",
      "   |     0.00s Max node = 117\n",
      "   |     0.00s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     0.01s node feature \"author\" with 1 node\n",
      "   |      |     0.01s node feature \"gap\" with 7 nodes\n",
      "   |      |     0.01s node feature \"letters\" with 99 nodes\n",
      "   |      |     0.01s node feature \"number\" with 17 nodes\n",
      "   |      |     0.01s node feature \"punc\" with 17 nodes\n",
      "   |      |     0.01s node feature \"terminator\" with 12 nodes\n",
      "   |      |     0.01s node feature \"title\" with 1 node\n",
      "   |     0.00s OK\n",
      "  0.00s Exporting 8 node and 1 edge and 1 config features to /Users/dirk/Downloads/banks/tf:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.00s maxSlot=         99\n",
      "  0.00s maxNode=        117\n",
      "  0.00s OK: oslots is valid\n",
      "   |     0.00s T author               to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T gap                  to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T letters              to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T number               to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T otype                to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T punc                 to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T terminator           to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T title                to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T oslots               to /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s M otext                to /Users/dirk/Downloads/banks/tf\n",
      "  0.02s Exported 8 node features and 1 edge features and 1 config features to /Users/dirk/Downloads/banks/tf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = cv.walk(\n",
    "    director,\n",
    "    slotType,\n",
    "    otext=otext,\n",
    "    generic=generic,\n",
    "    intFeatures=intFeatures,\n",
    "    featureMeta=featureMeta,\n",
    ")\n",
    "\n",
    "good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect\n",
    "\n",
    "Let's inspect some of the files:\n",
    "\n",
    "### otype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@node\n",
      "@compiler=Dirk Roorda\n",
      "@name=Culture quotes from Iain Banks\n",
      "@source=Good Reads\n",
      "@url=https://www.goodreads.com/work/quotes/14366-consider-phlebas\n",
      "@valueType=str\n",
      "@writtenBy=Text-Fabric\n",
      "@dateWritten=2019-01-30T15:15:20Z\n",
      "\n",
      "1-99\tword\n",
      "100\tbook\n",
      "101-102\tchapter\n",
      "103-114\tline\n",
      "115-117\tsentence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{TF_DIR}/otype.tf') as fh:\n",
    "  print(fh.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### otext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@config\n",
      "@compiler=Dirk Roorda\n",
      "@fmt:text-orig-full={letters}{punc} \n",
      "@name=Culture quotes from Iain Banks\n",
      "@sectionFeatures=title,number\n",
      "@sectionTypes=book,chapter\n",
      "@source=Good Reads\n",
      "@url=https://www.goodreads.com/work/quotes/14366-consider-phlebas\n",
      "@writtenBy=Text-Fabric\n",
      "@dateWritten=2019-01-30T15:15:20Z\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{TF_DIR}/otext.tf') as fh:\n",
    "  print(fh.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oslots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@edge\n",
      "@compiler=Dirk Roorda\n",
      "@name=Culture quotes from Iain Banks\n",
      "@source=Good Reads\n",
      "@url=https://www.goodreads.com/work/quotes/14366-consider-phlebas\n",
      "@valueType=str\n",
      "@writtenBy=Text-Fabric\n",
      "@dateWritten=2019-01-30T15:15:20Z\n",
      "\n",
      "100\t1-99\n",
      "1-55\n",
      "56-99\n",
      "1-3\n",
      "4-6\n",
      "7-9,14-20\n",
      "21-27\n",
      "28-38\n",
      "39-51\n",
      "52-55\n",
      "56\n",
      "57-75\n",
      "76-77,81-83\n",
      "84-88\n",
      "89-99\n",
      "1-27\n",
      "28-55\n",
      "56-99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{TF_DIR}/oslots.tf') as fh:\n",
    "  print(fh.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "* line `100\t1-99` tells that node 100 (the first book node, see *otext*, is linked to slots 1-99, which are all slots.\n",
    "* the next line has only `1-55`. These are the slots of node 101, being 1 + the previous node.\n",
    "\n",
    "And so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@node\n",
      "@compiler=Dirk Roorda\n",
      "@description=number of chapter, or sentence in chapter, or line in sentence\n",
      "@name=Culture quotes from Iain Banks\n",
      "@source=Good Reads\n",
      "@url=https://www.goodreads.com/work/quotes/14366-consider-phlebas\n",
      "@valueType=int\n",
      "@writtenBy=Text-Fabric\n",
      "@dateWritten=2019-01-30T15:15:20Z\n",
      "\n",
      "101\t1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "7\n",
      "8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{TF_DIR}/number.tf') as fh:\n",
    "  print(fh.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Node 101 is the first chapter node, which has chapter number 1.\n",
    "\n",
    "The next line is about node 102, the second chapter, with number 2.\n",
    "\n",
    "The following line refers to node 103, and a quick glance at the *otype* feature shows that this is a line.\n",
    "\n",
    "The last three lines are about the three sentences, which are numbered within their chapter:\n",
    "`1` then `2` and then again `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@node\n",
      "@compiler=Dirk Roorda\n",
      "@description=the letters of a word\n",
      "@name=Culture quotes from Iain Banks\n",
      "@source=Good Reads\n",
      "@url=https://www.goodreads.com/work/quotes/14366-consider-phlebas\n",
      "@valueType=str\n",
      "@writtenBy=Text-Fabric\n",
      "@dateWritten=2019-01-30T15:15:20Z\n",
      "\n",
      "Everything\n",
      "about\n",
      "us\n",
      "everything\n",
      "around\n",
      "us\n",
      "everything\n",
      "we\n",
      "know\n",
      "and\n",
      "can\n",
      "know\n",
      "of\n",
      "is\n",
      "composed\n",
      "ultimately\n",
      "of\n",
      "patterns\n",
      "of\n",
      "nothing\n",
      "that’s\n",
      "the\n",
      "bottom\n",
      "line\n",
      "the\n",
      "final\n",
      "truth\n",
      "So\n",
      "where\n",
      "we\n",
      "find\n",
      "we\n",
      "have\n",
      "any\n",
      "control\n",
      "over\n",
      "those\n",
      "patterns\n",
      "why\n",
      "not\n",
      "make\n",
      "the\n",
      "most\n",
      "elegant\n",
      "ones\n",
      "the\n",
      "most\n",
      "enjoyable\n",
      "and\n",
      "good\n",
      "ones\n",
      "in\n",
      "our\n",
      "own\n",
      "terms\n",
      "Besides\n",
      "it\n",
      "left\n",
      "the\n",
      "humans\n",
      "in\n",
      "the\n",
      "Culture\n",
      "free\n",
      "to\n",
      "take\n",
      "care\n",
      "of\n",
      "the\n",
      "things\n",
      "that\n",
      "really\n",
      "mattered\n",
      "in\n",
      "life\n",
      "such\n",
      "as\n",
      "sports\n",
      "games\n",
      "romance\n",
      "studying\n",
      "dead\n",
      "languages\n",
      "barbarian\n",
      "societies\n",
      "and\n",
      "impossible\n",
      "problems\n",
      "and\n",
      "climbing\n",
      "high\n",
      "mountains\n",
      "without\n",
      "the\n",
      "aid\n",
      "of\n",
      "a\n",
      "safety\n",
      "harness\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f'{TF_DIR}/letters.tf') as fh:\n",
    "  print(fh.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The plain, clean text of everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TF\n",
    "\n",
    "We are going to load the new data: all features.\n",
    "\n",
    "We start a new instance of the TF machinery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.3\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "10 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=TF_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ask for a list of all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('author',\n",
       " 'gap',\n",
       " 'letters',\n",
       " 'number',\n",
       " 'otype',\n",
       " 'punc',\n",
       " 'terminator',\n",
       " 'title',\n",
       " 'oslots')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allFeatures = TF.explore(silent=True, show=True)\n",
    "loadableFeatures = allFeatures['nodes'] + allFeatures['edges']\n",
    "loadableFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s T otype                from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T oslots               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T title                from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T number               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T letters              from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T punc                 from /Users/dirk/Downloads/banks/tf\n",
      "   |      |     0.00s C __levels__           from otype, oslots, otext\n",
      "   |      |     0.00s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.00s C __rank__             from otype, __order__\n",
      "   |      |     0.00s C __levUp__            from otype, oslots, __levels__, __rank__\n",
      "   |      |     0.00s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     0.00s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.00s C __sections__         from otype, oslots, otext, __levUp__, __levels__, title, number\n",
      "   |     0.00s T author               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T gap                  from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s T terminator           from /Users/dirk/Downloads/banks/tf\n",
      "  0.05s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load(loadableFeatures, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that all files are marked with a `T`.\n",
    "\n",
    "That means that Text-Fabric loads the features by reading the plain text `.tf` files.\n",
    "But after reading, it makes a binary equivalent and stores it as a `.tfx`\n",
    "file in the hidden `.tf` directory next to it.\n",
    "\n",
    "Furthermore, you see some lines marked with `C`. Here Text-Fabric is computing derived data,\n",
    "mostly about sections, the order of nodes, and the relative positions of nodes with respect to the slots they\n",
    "are linked to.\n",
    "\n",
    "The results of this pre-computation are also stored in that hidden `.tf` directory.\n",
    "\n",
    "The next time, Text-Fabric loads the data from their binary forms, which is much faster.\n",
    "And the pre-computation step will be skipped.\n",
    "\n",
    "If the binary files get outdated Text-Fabric will recompile and recompute everything automatically.\n",
    "\n",
    "So let's load again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.3\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "10 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s B otype                from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B oslots               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B title                from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B number               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B letters              from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B punc                 from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B author               from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B gap                  from /Users/dirk/Downloads/banks/tf\n",
      "   |     0.00s B terminator           from /Users/dirk/Downloads/banks/tf\n",
      "  0.03s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations=TF_DIR)\n",
    "api = TF.load(loadableFeatures, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where there were `T`s before, there are now `B`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hoisting\n",
    "\n",
    "We can access all TF data programmatically by using `api.Features`, or `api.F` (same thing) and a bunch of\n",
    "other API members. \n",
    "\n",
    "But if we working with a single data source, we can hoist those API members to the global namespace.\n",
    "\n",
    "This is not a thing to be done when you write modules for other people, but if you are the user yourself,\n",
    "why should not you make life just a little bit easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('ensureLoaded', 'TF', 'ignored', 'loadLog')),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Misc', 'messaging', ('cache', 'error', 'indent', 'info', 'reset')),\n",
       " ('Nodes',\n",
       "  'navigating-nodes',\n",
       "  ('N Nodes', 'sortKey', 'sortKeyTuple', 'otypeRank', 'sortNodes')),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, you have an overview of the names you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Finally, let's explore this set by means of Text-Fabric.\n",
    "\n",
    "### Frequency list\n",
    "\n",
    "We can get ordered frequency lists for the values of all features.\n",
    "\n",
    "First the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('the', 8),\n",
       " ('of', 5),\n",
       " ('and', 4),\n",
       " ('in', 3),\n",
       " ('we', 3),\n",
       " ('everything', 2),\n",
       " ('know', 2),\n",
       " ('most', 2),\n",
       " ('ones', 2),\n",
       " ('patterns', 2),\n",
       " ('us', 2),\n",
       " ('Besides', 1),\n",
       " ('Culture', 1),\n",
       " ('Everything', 1),\n",
       " ('So', 1),\n",
       " ('a', 1),\n",
       " ('about', 1),\n",
       " ('aid', 1),\n",
       " ('any', 1),\n",
       " ('around', 1),\n",
       " ('as', 1),\n",
       " ('barbarian', 1),\n",
       " ('bottom', 1),\n",
       " ('can', 1),\n",
       " ('care', 1),\n",
       " ('climbing', 1),\n",
       " ('composed', 1),\n",
       " ('control', 1),\n",
       " ('dead', 1),\n",
       " ('elegant', 1),\n",
       " ('enjoyable', 1),\n",
       " ('final', 1),\n",
       " ('find', 1),\n",
       " ('free', 1),\n",
       " ('games', 1),\n",
       " ('good', 1),\n",
       " ('harness', 1),\n",
       " ('have', 1),\n",
       " ('high', 1),\n",
       " ('humans', 1),\n",
       " ('impossible', 1),\n",
       " ('is', 1),\n",
       " ('it', 1),\n",
       " ('languages', 1),\n",
       " ('left', 1),\n",
       " ('life', 1),\n",
       " ('line', 1),\n",
       " ('make', 1),\n",
       " ('mattered', 1),\n",
       " ('mountains', 1),\n",
       " ('not', 1),\n",
       " ('nothing', 1),\n",
       " ('our', 1),\n",
       " ('over', 1),\n",
       " ('own', 1),\n",
       " ('problems', 1),\n",
       " ('really', 1),\n",
       " ('romance', 1),\n",
       " ('safety', 1),\n",
       " ('societies', 1),\n",
       " ('sports', 1),\n",
       " ('studying', 1),\n",
       " ('such', 1),\n",
       " ('take', 1),\n",
       " ('terms', 1),\n",
       " ('that', 1),\n",
       " ('that’s', 1),\n",
       " ('things', 1),\n",
       " ('those', 1),\n",
       " ('to', 1),\n",
       " ('truth', 1),\n",
       " ('ultimately', 1),\n",
       " ('where', 1),\n",
       " ('why', 1),\n",
       " ('without', 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.letters.freqList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the node types we can get info by calling this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('book', 99.0, 100, 100),\n",
       " ('chapter', 49.5, 101, 102),\n",
       " ('sentence', 33.0, 115, 117),\n",
       " ('line', 7.666666666666667, 103, 114),\n",
       " ('word', 1, 1, 99))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.levels.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means that chapters are 49.5 words long on average, and that the chapter nodes are 101 and 102.\n",
    "\n",
    "And you see that we have 99 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "Of course, there is much more to TF.\n",
    "\n",
    "Have a look through tutorials for several corpora: Hebrew and Syriac Bible, Quran, Uruk Cuneiform.\n",
    "\n",
    "Navigate from [here](https://nbviewer.jupyter.org/github/annotation/tutorials/tree/master/).\n",
    "\n",
    "Now conversion is this easy, more corpora will follow.\n",
    "\n",
    "The docs for conversion are [here](https://annotation.github.io/text-fabric/Create/Convert/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
